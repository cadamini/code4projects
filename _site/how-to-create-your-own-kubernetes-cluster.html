<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>How to create your own Kubernetes cluster</title>
    <link rel="stylesheet" href="/code4projects/assets/css/styles.css">
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/code4projects/feed.xml" title="Code4Projects" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>How to create your own Kubernetes cluster | Code4Projects</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="How to create your own Kubernetes cluster" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="What I have learned after 30 years of programming" />
<meta property="og:description" content="What I have learned after 30 years of programming" />
<link rel="canonical" href="http://localhost:4000/code4projects/how-to-create-your-own-kubernetes-cluster.html" />
<meta property="og:url" content="http://localhost:4000/code4projects/how-to-create-your-own-kubernetes-cluster.html" />
<meta property="og:site_name" content="Code4Projects" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="How to create your own Kubernetes cluster" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"What I have learned after 30 years of programming","headline":"How to create your own Kubernetes cluster","url":"http://localhost:4000/code4projects/how-to-create-your-own-kubernetes-cluster.html"}</script>
<!-- End Jekyll SEO tag -->

    
  </head>
  <body>
    <header>
  <div id="logo"></div>
  <div id="empty-header"></div>
  <div id="social-area"></div>
</header>

    <div id="menu">
        <nav>
  
    <a href="/code4projects/" >Home</a>
  
    <a href="/code4projects/about.html" >About</a>
  
    <a href="/code4projects/blog.html" >Blog</a>
  
    <a href="/code4projects/staff.html" >Staff</a>
  
</nav>

    </div>
    <div id="content">
        <h1 id="how-to-create-your-own-kubernetes-cluster">How to create your own Kubernetes cluster</h1>

<p>This is the fourth article of the <a href="/code4projects/">Getting Started with Kubernetes</a> article series. In this article, I want to explain how I run my applications on a Kubernetes cluster using a simple project based on <a href="https://www.vagrantup.com/">Vagrant</a> and <a href="https://www.virtualbox.org/">VirtualBox</a>. In order to test the cluster, we will create a “Hello K8s” application for Kubernetes.</p>

<h2 id="how-to-start">How to start?</h2>

<p>Almost all the tutorials on the Internet suggest starting using <strong>Minikube</strong>, a single node version of Kubernetes whose goal is to make life easier for those approaching the platform. The problem with Minikube is that it doesn’t allow you to prove the essence of Kubernetes, that is, orchestration on multiple nodes. With Minikube you can’t see what happens to your Pods when a node goes down.</p>

<p>Then there are tools like <a href="https://github.com/kubernetes-sigs/kubespray">Kubespray</a> that allow you to run, thanks to virtualization tools like Vagrant and VirtualBox, a cluster with multiple nodes on your development machine. This option if it is certainly valid when you are already familiar with Kubernetes, in the beginning, it abstracts many activities and does not allow you to understand what are the components that really serve your application and how they are installed.</p>

<p>For this reason, I use the <a href="https://github.com/sasadangelo/k8s-cluster/">k8s-cluster project</a> which allows you to create a cluster on your development machine thanks to Vagrant and VirtualBox. Here the commands to create the cluster:</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml">    <span class="s">git clone https://github.com/sasadangelo/k8s-cluster</span>
    <span class="s">cd k8s-cluster</span>
    <span class="s">vagrant up</span>
    </code></pre></figure>

<h2 id="configuration">Configuration</h2>

<p>The idea behind the k8s-cluster is to have a YAML configuration file where to describe the number of desired nodes and their characteristics. By default, we have 3 nodes having 2 CPUs and 2 Gb of RAM with Ubuntu 16.04 Xenial as the operating system.</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml">    <span class="s">---</span>
    <span class="s">- name</span><span class="err">:</span> <span class="s">k8s-head</span>
    <span class="s">type</span><span class="err">:</span> <span class="s">master</span>
    <span class="s">box</span><span class="err">:</span> <span class="s">ubuntu/xenial64</span>
    <span class="s">box_version</span><span class="err">:</span> <span class="s">20180831.0.0</span>
    <span class="s">eth1</span><span class="err">:</span> <span class="s">192.168.205.10</span>
    <span class="s">mem</span><span class="err">:</span> <span class="m">2048</span>
    <span class="na">cpu</span><span class="pi">:</span> <span class="m">2</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">k8s-node-1</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">node</span>
    <span class="na">box</span><span class="pi">:</span> <span class="s">ubuntu/xenial64</span>
    <span class="na">box_version</span><span class="pi">:</span> <span class="s">20180831.0.0</span>
    <span class="na">eth1</span><span class="pi">:</span> <span class="s">192.168.205.11</span>
    <span class="na">mem</span><span class="pi">:</span> <span class="m">2048</span>
    <span class="na">cpu</span><span class="pi">:</span> <span class="m">2</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">k8s-node-2</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">node</span>
    <span class="na">box</span><span class="pi">:</span> <span class="s">ubuntu/xenial64</span>
    <span class="na">box_version</span><span class="pi">:</span> <span class="s">20180831.0.0</span>
    <span class="na">eth1</span><span class="pi">:</span> <span class="s">192.168.205.12</span>
    <span class="na">mem</span><span class="pi">:</span> <span class="m">2048</span>
    <span class="na">cpu</span><span class="pi">:</span> <span class="m">2</span>
    </code></pre></figure>

<h2 id="how-does-it-work">How does it work?</h2>

<p>The YAML file is read by the Vagrantfile which instantiates the number of nodes on VirtualBox reported in the file with the described characteristics. Each node is then configured with three scripts:</p>

<ul>
  <li><strong>configure_box.sh</strong>, executed on all the cluster nodes;</li>
  <li><strong>configure_master.sh</strong>, executed only on the master node;</li>
  <li><strong>configure_workers.sh</strong>, executed only on the worker nodes.</li>
</ul>

<h3 id="vagrantfile">Vagrantfile</h3>

<p>The Vagrantfile is very simple because it reads the YAML file and, for each entry, generates a Server type object. It is important that the first is always the master. At this point, Vagrant loop on each Server object and instantiates a node on the VagrantBox. If the node is the master then it will execute the <strong>configure_box.sh</strong> and <strong>configure_master.sh</strong> scripts, otherwise <strong>configure_box.sh</strong> and <strong>configure_worker.sh</strong>.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">    <span class="nb">require</span> <span class="s1">'yaml'</span>

    <span class="c1"># Load settings from servers.yml file.</span>
    <span class="n">servers</span> <span class="o">=</span> <span class="no">YAML</span><span class="p">.</span><span class="nf">load_file</span><span class="p">(</span><span class="s1">'servers.yaml'</span><span class="p">)</span>

    <span class="no">Vagrant</span><span class="p">.</span><span class="nf">configure</span><span class="p">(</span><span class="s2">"2"</span><span class="p">)</span> <span class="k">do</span> <span class="o">|</span><span class="n">config</span><span class="o">|</span>
        <span class="n">servers</span><span class="p">.</span><span class="nf">each</span> <span class="k">do</span> <span class="o">|</span><span class="n">opts</span><span class="o">|</span>
            <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">define</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span> <span class="k">do</span> <span class="o">|</span><span class="n">config</span><span class="o">|</span>
                <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">box</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"box"</span><span class="p">]</span>
                <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">box_version</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"box_version"</span><span class="p">]</span>
                <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">hostname</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span>
                <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">network</span> <span class="ss">:private_network</span><span class="p">,</span> <span class="ss">ip: </span><span class="n">opts</span><span class="p">[</span><span class="s2">"eth1"</span><span class="p">]</span>

                <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">provider</span> <span class="s2">"virtualbox"</span> <span class="k">do</span> <span class="o">|</span><span class="n">v</span><span class="o">|</span>
                    <span class="n">v</span><span class="p">.</span><span class="nf">name</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span>
                    <span class="n">v</span><span class="p">.</span><span class="nf">customize</span> <span class="p">[</span><span class="s2">"modifyvm"</span><span class="p">,</span> <span class="ss">:id</span><span class="p">,</span> <span class="s2">"--groups"</span><span class="p">,</span> <span class="s2">"/K8s Development"</span><span class="p">]</span>
                    <span class="n">v</span><span class="p">.</span><span class="nf">customize</span> <span class="p">[</span><span class="s2">"modifyvm"</span><span class="p">,</span> <span class="ss">:id</span><span class="p">,</span> <span class="s2">"--memory"</span><span class="p">,</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"mem"</span><span class="p">]]</span>
                    <span class="n">v</span><span class="p">.</span><span class="nf">customize</span> <span class="p">[</span><span class="s2">"modifyvm"</span><span class="p">,</span> <span class="ss">:id</span><span class="p">,</span> <span class="s2">"--cpus"</span><span class="p">,</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"cpu"</span><span class="p">]]</span>

                <span class="k">end</span>

                <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">provision</span> <span class="s2">"shell"</span><span class="p">,</span> <span class="ss">path: </span><span class="s2">"configure_box.sh"</span><span class="p">,</span> <span class="ss">privileged: </span><span class="kp">true</span>
                <span class="k">if</span> <span class="n">opts</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"master"</span>
                    <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">provision</span> <span class="s2">"shell"</span><span class="p">,</span> <span class="ss">path: </span><span class="s2">"configure_master.sh"</span><span class="p">,</span> <span class="ss">privileged: </span><span class="kp">true</span>
                <span class="k">else</span>
                    <span class="n">config</span><span class="p">.</span><span class="nf">vm</span><span class="p">.</span><span class="nf">provision</span> <span class="s2">"shell"</span><span class="p">,</span> <span class="ss">path: </span><span class="s2">"configure_worker.sh"</span><span class="p">,</span> <span class="ss">privileged: </span><span class="kp">true</span>
                <span class="k">end</span>
            <span class="k">end</span>
        <span class="k">end</span>
    <span class="k">end</span>
    </code></pre></figure>

<h3 id="configure_boxsh-script">configure_box.sh script</h3>

<p>This script installs the following components on all three nodes:</p>

<ul>
  <li>Docker engine;</li>
  <li>kubeadm, the tool initializes the cluster on the master node and allows other nodes joining;</li>
  <li>kubectl, the Kubernetes command line interface (CLI);</li>
  <li>kubelet.</li>
</ul>

<p>Here the code to install the Docker engine:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    apt-get update
    apt-get <span class="nb">install</span> <span class="nt">-y</span> apt-transport-https ca-certificates curl software-properties-common
    curl <span class="nt">-fsSL</span> https://download.docker.com/linux/ubuntu/gpg | apt-key add -
    add-apt-repository <span class="s2">"deb https://download.docker.com/linux/</span><span class="si">$(</span><span class="nb">.</span> /etc/os-release<span class="p">;</span> <span class="nb">echo</span> <span class="s2">"</span><span class="nv">$ID</span><span class="s2">"</span><span class="si">)</span><span class="s2"> </span><span class="si">$(</span>lsb_release <span class="nt">-cs</span><span class="si">)</span><span class="s2"> stable"</span>
    apt-get update
    apt-get <span class="nb">install</span> <span class="nt">-y</span> docker-ce<span class="o">=</span><span class="si">$(</span>apt-cache madison docker-ce | <span class="nb">grep </span>17.03 | <span class="nb">head</span> <span class="nt">-1</span> | <span class="nb">awk</span> <span class="s1">'{print $3}'</span><span class="si">)</span>
    </code></pre></figure>

<p>Then the script adds the vagrant user to the docker group, in this way it can run docker commands.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    usermod <span class="nt">-aG</span> docker vagrant
    </code></pre></figure>

<p>The script installs kubectl, kubeadm, and kubelet using the following code:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    apt-get <span class="nb">install</span> <span class="nt">-y</span> apt-transport-https curl
    curl <span class="nt">-s</span> https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
    <span class="nb">cat</span> <span class="o">&gt;&gt;</span> /etc/apt/sources.list.d/kubernetes.list <span class="o">&lt;&lt;</span> <span class="no">EOL</span><span class="sh">
        deb http://apt.kubernetes.io/ kubernetes-xenial main
</span><span class="no">    EOL
</span>    apt-get update
    apt-get <span class="nb">install</span> <span class="nt">-y</span> kubelet kubeadm kubectl
    apt-mark hold kubelet kubeadm kubectl
    swapoff <span class="nt">-a</span>
    <span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'/ swap / s/^\(.*\)$/#\1/g'</span> /etc/fstab
    <span class="nv">IP_ADDR</span><span class="o">=</span><span class="sb">`</span>ifconfig enp0s8 | <span class="nb">grep </span>Mask | <span class="nb">awk</span> <span class="s1">'{print $2}'</span> | <span class="nb">cut</span> <span class="nt">-f2</span> <span class="nt">-d</span>:<span class="sb">`</span>
    <span class="nb">echo</span> <span class="s2">"KUBELET_EXTRA_ARGS=--node-ip=</span><span class="nv">$IP_ADDR</span><span class="s2">"</span> | <span class="nb">tee</span> <span class="nt">-a</span> /etc/default/kubelet
    systemctl restart kubelet
    </code></pre></figure>

<h3 id="configure_mastersh-script">configure_master.sh script</h3>

<p>This script performs the following actions:</p>

<ul>
  <li>initialize the cluster;</li>
  <li>configure vagrant user to use kubectl commands;</li>
  <li>install the Calico network plugin;</li>
  <li>generate the join script to run on the worker nodes;</li>
  <li>configure ssh to allow password authentication.</li>
</ul>

<p>This is the code to initialize the cluster:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    <span class="nv">IP_ADDR</span><span class="o">=</span><span class="sb">`</span>ifconfig enp0s8 | <span class="nb">grep </span>Mask | <span class="nb">awk</span> <span class="s1">'{print $2}'</span>| <span class="nb">cut</span> <span class="nt">-f2</span> <span class="nt">-d</span>:<span class="sb">`</span>
    <span class="nv">HOST_NAME</span><span class="o">=</span><span class="si">$(</span><span class="nb">hostname</span> <span class="nt">-s</span><span class="si">)</span>
    kubeadm init <span class="nt">--apiserver-advertise-address</span><span class="o">=</span><span class="nv">$IP_ADDR</span> <span class="nt">--apiserver-cert-extra-sans</span><span class="o">=</span><span class="nv">$IP_ADDR</span>  <span class="nt">--node-name</span> <span class="nv">$HOST_NAME</span> <span class="nt">--pod-network-cidr</span><span class="o">=</span>172.16.0.0/16
    </code></pre></figure>

<p>The second step configures the vagrant user to use kubectl commands:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    <span class="nb">sudo</span> <span class="nt">--user</span><span class="o">=</span>vagrant <span class="nb">mkdir</span> <span class="nt">-p</span> /home/vagrant/.kube
    <span class="nb">cp</span> <span class="nt">-i</span> /etc/kubernetes/admin.conf /home/vagrant/.kube/config
    <span class="nb">chown</span> <span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span> vagrant<span class="si">)</span>:<span class="si">$(</span><span class="nb">id</span> <span class="nt">-g</span> vagrant<span class="si">)</span> /home/vagrant/.kube/config
    </code></pre></figure>

<p>The third step installs the Calico network plugin:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    <span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span>/etc/kubernetes/admin.conf
    kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/sasadangelo/k8s-cluster/master/calico/rbac-kdd.yaml
    kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/sasadangelo/k8s-cluster/master/calico/calico.yaml
    </code></pre></figure>

<p>Then the script generates the joining script to run on the worker nodes:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubeadm token create <span class="nt">--print-join-command</span> 2&gt;/dev/null | <span class="nb">tee</span> <span class="nt">-a</span> /etc/kubeadm_join_cmd.sh
    <span class="nb">chmod</span> +x /etc/kubeadm_join_cmd.sh
    </code></pre></figure>

<p>Finally, the script configures ssh to enable password authentication:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    <span class="nb">sed</span> <span class="nt">-i</span> <span class="s2">"/^[^#]*PasswordAuthentication[[:space:]]no/c</span><span class="se">\P</span><span class="s2">asswordAuthentication yes"</span> /etc/ssh/sshd_config
    service sshd restart
    </code></pre></figure>

<h3 id="configure_workersh-script">configure_worker.sh script</h3>

<p>On the worker node, the only step performed is a copy of the joining script from the master node and its execution to let the worker node join the cluster.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    apt-get update
    apt-get <span class="nb">install</span> <span class="nt">-y</span> sshpass
    sshpass <span class="nt">-p</span> <span class="s2">"vagrant"</span> scp <span class="nt">-o</span> <span class="nv">StrictHostKeyChecking</span><span class="o">=</span>no vagrant@192.168.205.10:/etc/kubeadm_join_cmd.sh <span class="nb">.</span>
    sh ./kubeadm_join_cmd.sh
    </code></pre></figure>

<h2 id="control-the-cluster-from-your-laptop">Control the cluster from your laptop</h2>

<p>Currently to manage the cluster you need to access your Vagrant machines via ssh to use kubectl commands. You can avoid this installing kubectl on your local machine and use it to control your cluster.</p>

<p>To do that you need to install kubectl on your machine following this guide. Then you need to copy the Kubernetes credentials from your remote host:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    <span class="nb">cd</span> ~
    scp <span class="nt">-r</span> vagrant@192.168.205.10:/home/vagrant/.kube <span class="nb">.</span>
    </code></pre></figure>

<p>Running the kubectl get pods command, you should see the cluster nodes.</p>

<h2 id="how-to-create-a-hello-k8s-application">How to create a “Hello K8s!” application</h2>

<p>This is your first Kubernetes “Hello World” application. It is an Nginx web server that listens on 80 port and when you connect to it with your browser the “Hello World!” message will appear with hostname and image version. This will be useful to understand which Pod responded to a browser request and its hostname and which version is currently in use.</p>

<p>In the <a href="/code4projects/">following article</a>, I created a Hello World application for Docker that we will reuse for Kubernetes with small changes. Here the Dockerfile.</p>

<figure class="highlight"><pre><code class="language-docker" data-lang="docker">    FROM ubuntu:16.04
    RUN apt-get update; apt-get install -y nginx php7.0 php7.0-gd php7.0-mysql php7.0-curl vim
    COPY nginx.conf /etc/nginx/nginx.conf
    COPY ./www-data /home/www/www-data
    COPY entrypoint.sh .
    COPY VERSION .
    COPY VERSION /home/www/www-data
    EXPOSE 80
    ENTRYPOINT /entrypoint.sh
    </code></pre></figure>

<p>As you can notice, in this Dockerfile we install PHP in addition to Nginx to run the <strong>index.php</strong> file in the <strong>www-data</strong> folder. The reason why we use a PHP file instead of an HTML one is that we want to print the version of the application and the hostname in order to know which version of the application we are running and on which Pod.</p>

<p>The <strong>ENTRYPOINT</strong> of the Docker container is the <strong>entrypoint.sh</strong> script that set the right permission for the <strong>/var/log/nginx</strong> folder and it will start the <strong>php7.0-fpm</strong> and the nginx services. You can check out the source code here.</p>

<p>The docker image of this application is now on my Docker Hub account <a href="https://hub.docker.com/repository/docker/sasadangelo/hello-k8s">sasadangelo/hello-k8s</a>.</p>

<h2 id="how-to-run-the-hello-k8s-application">How to run the “Hello K8s!” application</h2>

<p>Kubernetes allows running a containerized application in three approaches: generators, imperative, and declarative. The first two methods are achieved via <strong>kubectl</strong> CLI while the third method is achieved declaring the desired state in a YAML configuration file. In all the cases, the result is this.</p>

<p>IMAGE</p>

<p>Let’s analyze all these methods in detail.</p>

<h3 id="generators">Generators</h3>

<p>This is the easiest method and it is achieved using the <strong>kubect run</strong> and <strong>kubectl expose</strong> commands.  It is useful when you want to run a quick test just to check if the application works. Since no deployment is created behind the scene you cannot scale the Pod.</p>

<p>The command to run the application is:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl run hello-k8s <span class="nt">--generator</span><span class="o">=</span>run-pod/v1 <span class="nt">--image</span><span class="o">=</span>sasadangelo/hello-k8s:latest <span class="nt">--port</span><span class="o">=</span>80
    </code></pre></figure>

<p>Check if the Pod is running typing the kubectl get pods command. In order to connect with the browser from your host machine, you need to expose the Pod via Service using the following command:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl expose pod hello-k8s <span class="nt">--type</span><span class="o">=</span>NodePort <span class="nt">--port</span><span class="o">=</span>80
    </code></pre></figure>

<p>You can type now in your browser the URL ÌP:PORT, where IP is the 192.168.x.x address of one of the two worker nodes (<strong>k8s-node-1</strong> or <strong>k8s-node-2</strong>) and PORT is the one you get typing the command:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl describe service hello-k8s | <span class="nb">grep </span>NodePort
    </code></pre></figure>

<p>Clean up the configuration using the commands:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl delete service hello-k8s
    kubectl delete pod hello-k8s
    </code></pre></figure>

<h3 id="imperative">Imperative</h3>

<p>This method is achieved using the commands kubect create and kubectl expose. The first command creates a deployment behind the scene so you can scale the Pos as you prefer. The command to deploy and run the application is:</p>

<p>kubectl create deployment hello-k8s –image=sasadangelo/hello-k8s:latest
Check if the Deployment is created and the Pod is running typing the <strong>kubectl get deployments</strong> and <strong>kubectl get pods</strong> commands.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl get deployments
    kubectl get pods
    </code></pre></figure>

<p>In order to connect with the browser from your host machine you need to expose the Deployment via Service using the following command:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl expose deployment hello-k8s <span class="nt">--type</span><span class="o">=</span>NodePort <span class="nt">--port</span><span class="o">=</span>80
    </code></pre></figure>

<p>You can type now in your browser the URL ÌP:PORT, where IP is the 192.168.x.x address of one of the two worker nodes ( k8s-node-1 or k8s-node-2 ) and PORT is the one you get typing the command:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl describe service hello-k8s | <span class="nb">grep </span>NodePort
    </code></pre></figure>

<p>Scale the application to 5 pods with the following commands:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl scale <span class="nt">--replicas</span><span class="o">=</span>5 deployment.apps/hello-k8s
    </code></pre></figure>

<p>See the 5 pods running using the kubectl get pods command. If you type your browser Reload button continuously you can notice sometimes the hostname change because different pods will respond. Attention!!! It could be possible you have to type the Reload button a lot of time before see the hostname change due to Pod affinity.</p>

<p>Clean up the configuration using the commands:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl delete service hello-k8s
    kubectl delete deployment deployment.apps/hello-k8s
    </code></pre></figure>

<h3 id="declarative">Declarative</h3>

<p>This method is achieved using the commands kubect apply. This command uses a deployment file where is defined as the deployment and the service resource objects.</p>

<p>The command to deploy and run the application is:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl apply <span class="nt">-f</span> https://raw.githubusercontent.com/sasadangelo/k8s-tutorials/master/hello-k8s/deployment.yml
    </code></pre></figure>

<p>You can see 5 pods running using the kubectl get pods command. You can type now in your browser the URL ÌP:PORT, where IP is the 192.168.x.x address of one of the two worker nodes ( k8s-node-1 or k8s-node-2 ) and PORT is the one you get typing the command:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">    kubectl describe service hello-k8s-service <span class="nt">-n</span> hello-k8s-ns | <span class="nb">grep </span>NodePort
    </code></pre></figure>

<p>Clean up the configuration using the commands:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell">kubectl delete service hello-k8s-service <span class="nt">-n</span> hello-k8s-ns
kubectl delete deployment hello-k8s-deployment <span class="nt">-n</span> hello-k8s-ns
kubectl delete namespace hello-k8s-ns
    </code></pre></figure>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>In this article, we started playing with Kubernetes creating our own cluster and deploy a “Hello World” application using different approaches. In the next articles, we will explore more on Kubernetes running more complex applications.</p>

    </div>
    <div id="course_sidebar">
        <nav>
  <ul>
  
    <li><a href="/code4projects/" >Getting started with Kubernetes</a></li>
  
    <li><a href="/code4projects/kubernetes-services-cluster-ip-vs-nodeport-vs-loadbalancer-vs-ingress.html" >Kubernetes Cluster IP vs NodePort vs LoadBalancer vs Ingress</a></li>
  
    <li><a href="/code4projects/how-to-use-kubernetes-configmaps.html" >How I use Kubernetes ConfigMaps to manage configurations</a></li>
  
    <li><a href="/code4projects/how-to-create-your-own-kubernetes-cluster.html" class="current">How to create your own Kubernetes cluster</a></li>
  
  </ul>
</nav>

    </div>
  </body>
</html>
